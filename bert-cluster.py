# Cluster GitHub issues
# usage: python3 bert-cluster path/to/input.pkl
# TODO a way of combining multiple dataframes generated from different GitHub repos

import sys
import os
import random
import numpy as np
import pandas as pd

from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_samples, silhouette_score

IN_PKL = sys.argv[1]
N_CLUSTERS = 2

SEED = 42
random.seed(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)
np.random.seed(SEED)

# read in pickle of dataframe generated by csv-to-bert.py
df = pd.read_pickle(IN_PKL)
docs = df["text"].values
vectorized_docs = np.vstack(df["bert_vector"])

# ChatGPT generated stuff that I haven't been able to verify the efficacy of yet because vectorizing takes ages - mz

# Clustering
def mbkmeans_clusters(X, k, mb, print_silhouette_values):
    """Generate clusters and print Silhouette metrics using MiniBatchKMeans.

    Args:
        X: Matrix of features.
        k: Number of clusters.
        mb: Size of mini-batches.
        print_silhouette_values: Print silhouette values per cluster.

    Returns:
        Trained clustering model and labels based on X.
    """
    km = MiniBatchKMeans(n_clusters=k, batch_size=mb, random_state=SEED).fit(X)
    print(f"For n_clusters = {k}")
    print(f"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}")
    print(f"Inertia: {km.inertia_}")

    if print_silhouette_values:
        sample_silhouette_values = silhouette_samples(X, km.labels_)
        print(f"Silhouette values:")
        silhouette_values = []
        for i in range(k):
            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]
            silhouette_values.append(
                (
                    i,
                    cluster_silhouette_values.shape[0],
                    cluster_silhouette_values.mean(),
                    cluster_silhouette_values.min(),
                    cluster_silhouette_values.max(),
                )
            )
        silhouette_values = sorted(
            silhouette_values, key=lambda tup: tup[2], reverse=True
        )
        for s in silhouette_values:
            print(
                f"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}"
            )
    return km, km.labels_, km.cluster_centers_

print("Clustering documents...")
clustering, cluster_labels, centroids = mbkmeans_clusters(
    X=vectorized_docs,
    k=N_CLUSTERS,
    mb=500,
    print_silhouette_values=True,
)

# Save clustering results
df_clusters = pd.DataFrame({
    "text": docs,
    "cluster": cluster_labels
})
print(df_clusters.head())

for i in range(clustering.n_clusters):
    # Compute distances from each document to the cluster centroid
    distances = np.linalg.norm(vectorized_docs - clustering.cluster_centers_[i], axis=1)
    
    # Find the indices of the closest documents to the centroid
    closest_indices = np.argsort(distances)[:5]
    
    print(f"Cluster {i}:")
    for idx in closest_indices:
        print(f"    {docs[idx]}")
    print("-------------")
